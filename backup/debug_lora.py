#!/usr/bin/env python3
"""
LoRA Configuration Debug Script
è°ƒè¯•LoRAé…ç½®ä¸ºä»€ä¹ˆä¸ç”Ÿæ•ˆçš„ä¸“ç”¨è„šæœ¬
"""

import os
import sys
import torch
from pathlib import Path

# è®¾ç½®å•GPUç¯å¢ƒ
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

def print_section(title):
    print(f"\n{'='*60}")
    print(f" {title}")
    print(f"{'='*60}")

def check_lora_support():
    """æ£€æŸ¥LoRAç›¸å…³åº“å’Œæ”¯æŒæƒ…å†µ"""
    print_section("LoRA SUPPORT CHECK")

    # æ£€æŸ¥PEFTåº“
    try:
        import peft
        print(f"âœ… PEFT library available: {peft.__version__}")

        # æ£€æŸ¥æ”¯æŒçš„LoRAé…ç½®
        from peft import LoraConfig, get_peft_model
        print("âœ… LoRA classes imported successfully")

        # æ£€æŸ¥å¯ç”¨çš„ä»»åŠ¡ç±»å‹
        from peft import TaskType
        print(f"âœ… Available task types: {list(TaskType)}")

    except ImportError as e:
        print(f"âŒ PEFT library not available: {e}")
        print("ğŸ’¡ Install with: pip install peft")
        return False

    return True

def check_transformers_lora():
    """æ£€æŸ¥transformerså†…ç½®çš„LoRAæ”¯æŒ"""
    print_section("TRANSFORMERS LORA CHECK")

    try:
        from transformers import TrainingArguments

        # åˆ›å»ºè®­ç»ƒå‚æ•°çœ‹æ˜¯å¦æ”¯æŒLoRA
        dummy_args = TrainingArguments(
            output_dir="./temp",
            lora_enable=True,
            lora_r=32,
            lora_alpha=16,
        )

        print("âœ… TrainingArguments accepts LoRA parameters")
        print(f"   lora_enable: {dummy_args.lora_enable}")
        print(f"   lora_r: {dummy_args.lora_r}")
        print(f"   lora_alpha: {dummy_args.lora_alpha}")

        return True

    except Exception as e:
        print(f"âŒ TrainingArguments LoRA support issue: {e}")
        return False

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½å’ŒLoRAé…ç½®"""
    print_section("MODEL LOADING TEST")

    MODEL_NAME = "./models/Qwen2.5-VL-7B-Instruct"

    if not os.path.exists(MODEL_NAME):
        print(f"âŒ Model not found at: {MODEL_NAME}")
        return None

    try:
        from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer

        print("ğŸ”„ Loading model (this may take a moment)...")
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.bfloat16,
            device_map="cpu",  # åŠ è½½åˆ°CPUé¿å…æ˜¾å­˜é—®é¢˜
        )

        print("âœ… Model loaded successfully")

        # æ£€æŸ¥æ¨¡å‹ç»“æ„
        total_params = sum(p.numel() for p in model.parameters())
        print(f"ğŸ“Š Total parameters: {total_params:,}")

        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒLoRA
        print("\nğŸ” Checking model structure for LoRA compatibility...")

        # æŸ¥æ‰¾çº¿æ€§å±‚ï¼ˆLoRAé€šå¸¸åº”ç”¨äºçº¿æ€§å±‚ï¼‰
        linear_layers = []
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Linear):
                linear_layers.append(name)

        print(f"ğŸ“‹ Found {len(linear_layers)} linear layers")
        if len(linear_layers) <= 10:
            for layer in linear_layers:
                print(f"   - {layer}")
        else:
            print(f"   - {linear_layers[0]} ... {linear_layers[-1]} (showing first and last)")

        return model

    except Exception as e:
        print(f"âŒ Model loading failed: {e}")
        return None

def test_peft_lora(model):
    """æµ‹è¯•PEFTåº“çš„LoRAé…ç½®"""
    print_section("PEFT LORA TEST")

    if model is None:
        print("âŒ No model available for PEFT testing")
        return

    try:
        from peft import LoraConfig, get_peft_model, TaskType

        # åˆ›å»ºLoRAé…ç½®
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,  # æˆ–è€…å…¶ä»–é€‚åˆçš„ä»»åŠ¡ç±»å‹
            r=32,
            lora_alpha=16,
            lora_dropout=0.05,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],  # å¸¸è§çš„æ³¨æ„åŠ›å±‚
        )

        print("âœ… LoRA config created")
        print(f"   rank: {lora_config.r}")
        print(f"   alpha: {lora_config.lora_alpha}")
        print(f"   target_modules: {lora_config.target_modules}")

        # å°è¯•åº”ç”¨LoRA
        try:
            peft_model = get_peft_model(model, lora_config)

            # æ£€æŸ¥å‚æ•°ç»Ÿè®¡
            trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)
            total_params = sum(p.numel() for p in peft_model.parameters())

            print("âœ… PEFT LoRA model created successfully!")
            print(f"ğŸ“Š Parameter Statistics:")
            print(f"   Trainable: {trainable_params:,}")
            print(f"   Total: {total_params:,}")
            print(f"   Ratio: {trainable_params/total_params*100:.2f}%")

            if trainable_params < total_params * 0.1:  # å°‘äº10%è¯´æ˜LoRAç”Ÿæ•ˆ
                print("âœ… LoRA working correctly - only small fraction trainable!")
            else:
                print("âš ï¸ LoRA might not be working - too many trainable parameters")

        except Exception as e:
            print(f"âŒ PEFT model creation failed: {e}")

    except ImportError:
        print("âŒ PEFT library not available")

def test_transformers_integration():
    """æµ‹è¯•transformerså’ŒLoRAçš„é›†æˆ"""
    print_section("TRANSFORMERS INTEGRATION TEST")

    # æ£€æŸ¥æˆ‘ä»¬çš„è®­ç»ƒå‚æ•°é…ç½®
    sys.path.append(str(Path(__file__).parent))

    try:
        from qwenvl.train.argument import TrainingArguments

        print("ğŸ”„ Testing our TrainingArguments class...")

        args = TrainingArguments(
            output_dir="./temp",
            lora_enable=True,
            lora_r=32,
            lora_alpha=16,
            lora_dropout=0.05,
        )

        print("âœ… Our TrainingArguments works with LoRA")
        print(f"   lora_enable: {args.lora_enable}")
        print(f"   lora_r: {args.lora_r}")

        # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªå®šä¹‰çš„LoRAåº”ç”¨é€»è¾‘
        from qwenvl.train import trainer
        print("âœ… Trainer module imported")

        # æ£€æŸ¥æ˜¯å¦æœ‰LoRAç›¸å…³çš„å‡½æ•°
        trainer_functions = [name for name in dir(trainer) if 'lora' in name.lower()]
        if trainer_functions:
            print(f"ğŸ” Found LoRA-related functions: {trainer_functions}")
        else:
            print("âš ï¸ No LoRA-related functions found in trainer module")

    except Exception as e:
        print(f"âŒ Integration test failed: {e}")

def main():
    print_section("LORA DEBUG SCRIPT STARTED")
    print("This script will diagnose why LoRA is not working properly")

    # åŸºç¡€ç¯å¢ƒæ£€æŸ¥
    print(f"ğŸ Python version: {sys.version}")
    print(f"ğŸ”¥ PyTorch version: {torch.__version__}")
    print(f"ğŸ¤— Transformers version: {__import__('transformers').__version__}")

    # 1. æ£€æŸ¥LoRAæ”¯æŒ
    peft_available = check_lora_support()

    # 2. æ£€æŸ¥transformers LoRAæ”¯æŒ
    transformers_lora_ok = check_transformers_lora()

    # 3. æµ‹è¯•æ¨¡å‹åŠ è½½
    model = test_model_loading()

    # 4. æµ‹è¯•PEFT LoRAï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if peft_available:
        test_peft_lora(model)

    # 5. æµ‹è¯•é›†æˆ
    test_transformers_integration()

    print_section("DEBUG SUMMARY")
    print("Key Findings:")
    print(f"  PEFT Available: {'âœ…' if peft_available else 'âŒ'}")
    print(f"  Transformers LoRA: {'âœ…' if transformers_lora_ok else 'âŒ'}")
    print(f"  Model Loaded: {'âœ…' if model is not None else 'âŒ'}")

    if not peft_available:
        print("\nğŸ’¡ Recommendation: Install PEFT library")
        print("   pip install peft")

    print("\nğŸ”š Debug script completed!")

if __name__ == "__main__":
    main()